<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Modern Transformer Block Architecture — Corrected</title>
<style>
  * { margin: 0; padding: 0; box-sizing: border-box; }
  body { background: #0a0a0f; color: #e0e0e0; font-family: 'Segoe UI', system-ui, sans-serif; display: flex; justify-content: center; padding: 30px 20px; min-height: 100vh; }
  .container { max-width: 1100px; width: 100%; }
  h1 { text-align: center; font-size: 1.5rem; color: #fff; margin-bottom: 6px; }
  .subtitle { text-align: center; color: #888; font-size: 0.85rem; margin-bottom: 10px; }
  
  .tab-bar { display: flex; justify-content: center; gap: 8px; margin-bottom: 24px; flex-wrap: wrap; }
  .tab-btn { background: #1a1a2e; border: 1px solid #333; color: #aaa; padding: 8px 18px; border-radius: 6px; cursor: pointer; font-size: 0.85rem; transition: all 0.2s; }
  .tab-btn:hover { border-color: #666; color: #fff; }
  .tab-btn.active { background: #2a2a4e; border-color: #4a6cf7; color: #fff; }
  .tab-content { display: none; }
  .tab-content.active { display: block; }
  
  svg { width: 100%; height: auto; display: block; }
  
  .block { rx: 8; }
  .label { fill: #fff; font-size: 13px; font-weight: 600; text-anchor: middle; dominant-baseline: central; }
  .sublabel { fill: #aaa; font-size: 10px; text-anchor: middle; dominant-baseline: central; }
  .arrow { stroke: #555; stroke-width: 1.5; fill: none; marker-end: url(#ah); }
  .arrow-skip { stroke: #4af7f7; stroke-width: 1.5; stroke-dasharray: 6,3; fill: none; marker-end: url(#ah-skip); }
  .annotation { fill: #888; font-size: 10px; text-anchor: start; }
  .section-label { fill: #666; font-size: 11px; text-anchor: start; font-weight: 600; letter-spacing: 1px; text-transform: uppercase; }
  .bracket { stroke: #444; stroke-width: 1; fill: none; }
  
  .legend { margin-top: 20px; display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 6px; }
  .legend-item { display: flex; align-items: center; gap: 8px; font-size: 0.78rem; color: #aaa; }
  .legend-color { width: 14px; height: 14px; border-radius: 3px; flex-shrink: 0; }
  
  .note { margin-top: 16px; background: #111; border: 1px solid #333; border-radius: 8px; padding: 14px; font-size: 0.82rem; line-height: 1.6; color: #bbb; }
  .note strong { color: #4af7f7; }
  .note em { color: #f7c94a; }
  .note code { background: #1a1a2e; padding: 1px 5px; border-radius: 3px; font-size: 0.8rem; color: #e0e0e0; }
  .note .warn { color: #f74a4a; }
  
  .model-table { width: 100%; border-collapse: collapse; margin-top: 16px; font-size: 0.8rem; }
  .model-table th { background: #1a1a2e; color: #aaa; padding: 8px 12px; text-align: left; border-bottom: 1px solid #333; font-weight: 600; }
  .model-table td { padding: 7px 12px; border-bottom: 1px solid #1a1a1a; color: #ccc; }
  .model-table tr:hover td { background: #111; }
  .model-table .highlight { color: #4cf74a; font-weight: 600; }
  .model-table .outdated { color: #f74a4a; }
  
  .comparison { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin-top: 20px; }
  .comparison > div { background: #111; border: 1px solid #333; border-radius: 8px; padding: 14px; }
  .comparison h3 { font-size: 0.9rem; margin-bottom: 10px; }
  @media (max-width: 700px) { .comparison { grid-template-columns: 1fr; } }
</style>
</head>
<body>
<div class="container">
  <h1>Modern Transformer Block Architecture</h1>
  <p class="subtitle">Pre-LN with SwiGLU FFN — As used in LLaMA, Mistral, PaLM, Gemma</p>

  <div class="tab-bar">
    <button class="tab-btn active" onclick="showTab('modern')">① Modern Pre-LN (LLaMA-style)</button>
    <button class="tab-btn" onclick="showTab('swiglu')">② SwiGLU FFN Detail</button>
    <button class="tab-btn" onclick="showTab('compare')">③ Pre-LN vs Post-LN</button>
    <button class="tab-btn" onclick="showTab('activations')">④ Activation Functions</button>
  </div>

  <!-- ============ TAB 1: MODERN PRE-LN ============ -->
  <div id="tab-modern" class="tab-content active">
    <svg viewBox="0 0 750 960" xmlns="http://www.w3.org/2000/svg">
      <defs>
        <marker id="ah" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto"><polygon points="0 0, 8 3, 0 6" fill="#555"/></marker>
        <marker id="ah-skip" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto"><polygon points="0 0, 8 3, 0 6" fill="#4af7f7"/></marker>
      </defs>

      <!-- Input -->
      <rect x="200" y="20" width="300" height="44" fill="#2e1a2e" stroke="#c74af7" stroke-width="1.5" rx="8"/>
      <text x="350" y="36" class="label">Input Embedding</text>
      <text x="350" y="52" class="sublabel">Token Embed + RoPE Positional Encoding</text>

      <line x1="350" y1="64" x2="350" y2="100" class="arrow"/>

      <!-- ===== ATTENTION SUB-BLOCK (Pre-LN) ===== -->
      <text x="30" y="108" class="section-label">Attention</text>
      <text x="30" y="122" class="section-label">Sub-Block</text>
      <path d="M 22,95 L 16,95 L 16,290 L 22,290" class="bracket"/>

      <!-- RMSNorm BEFORE attention (Pre-LN) -->
      <rect x="230" y="95" width="240" height="34" fill="#2e2a1a" stroke="#f7c94a" stroke-width="1.5" rx="8"/>
      <text x="350" y="112" class="label" style="font-size:12px">RMSNorm</text>
      <text x="485" y="112" class="annotation" style="fill:#f7c94a">← Pre-LN: normalize BEFORE attention</text>

      <line x1="350" y1="129" x2="350" y2="155" class="arrow"/>

      <!-- Multi-Head GQA -->
      <rect x="200" y="155" width="300" height="60" fill="#1a1a3e" stroke="#4a6cf7" stroke-width="1.5" rx="8"/>
      <text x="350" y="175" class="label">Multi-Head Attention (GQA)</text>
      <text x="350" y="195" class="sublabel">Grouped-Query Attention · RoPE · KV Cache compatible</text>

      <!-- Skip connection 1 -->
      <line x1="200" y1="95" x2="130" y2="95" class="arrow-skip" style="marker-end:none"/>
      <line x1="130" y1="95" x2="130" y2="250" class="arrow-skip" style="marker-end:none"/>
      <line x1="130" y1="250" x2="200" y2="250" class="arrow-skip"/>
      <text x="80" y="170" style="fill:#4af7f7; font-size:9px; text-anchor:middle; font-weight:600">Residual</text>
      <text x="80" y="182" style="fill:#4af7f7; font-size:9px; text-anchor:middle; font-weight:600">Skip ①</text>
      <text x="80" y="197" style="fill:#4af7f7; font-size:8px; text-anchor:middle">∂L/∂x = 1</text>

      <line x1="350" y1="215" x2="350" y2="240" class="arrow"/>

      <!-- Add 1 -->
      <rect x="280" y="240" width="140" height="30" fill="#1a2e2e" stroke="#4af7f7" stroke-width="1.5" rx="8"/>
      <text x="350" y="255" class="label" style="font-size:11px">⊕ Add (x + Attn(x))</text>

      <line x1="350" y1="270" x2="350" y2="320" class="arrow"/>

      <!-- ===== FFN SUB-BLOCK (Pre-LN) ===== -->
      <text x="30" y="340" class="section-label">FFN</text>
      <text x="30" y="354" class="section-label">Sub-Block</text>
      <path d="M 22,315 L 16,315 L 16,630 L 22,630" class="bracket"/>

      <!-- RMSNorm BEFORE FFN -->
      <rect x="230" y="315" width="240" height="34" fill="#2e2a1a" stroke="#f7c94a" stroke-width="1.5" rx="8"/>
      <text x="350" y="332" class="label" style="font-size:12px">RMSNorm</text>
      <text x="485" y="332" class="annotation" style="fill:#f7c94a">← Pre-LN: normalize BEFORE FFN</text>

      <line x1="350" y1="349" x2="350" y2="378" class="arrow"/>

      <!-- SwiGLU Gate -->
      <rect x="190" y="378" width="320" height="50" fill="#1a2e1a" stroke="#4cf74a" stroke-width="1.5" rx="8"/>
      <text x="350" y="396" class="label" style="font-size:12px">SwiGLU Gate</text>
      <text x="350" y="414" class="sublabel">SiLU(xW_gate) ⊙ (xW_up)  — gated linear unit with Swish</text>

      <text x="525" y="390" class="annotation" style="fill:#4cf74a">← Gated activation:</text>
      <text x="525" y="404" class="annotation" style="fill:#4cf74a">   smooth, no dead neurons,</text>
      <text x="525" y="418" class="annotation" style="fill:#4cf74a">   better gradient flow than ReLU</text>

      <line x1="350" y1="428" x2="350" y2="460" class="arrow"/>

      <!-- Down projection -->
      <rect x="220" y="460" width="260" height="40" fill="#1a2e1a" stroke="#4cf74a" stroke-width="1.5" rx="8"/>
      <text x="350" y="474" class="label" style="font-size:12px">Linear (Down Projection)</text>
      <text x="350" y="488" class="sublabel">W_down · hidden → d_model</text>

      <line x1="350" y1="500" x2="350" y2="530" class="arrow"/>

      <!-- Dropout (optional) -->
      <rect x="275" y="530" width="150" height="28" fill="#1a1a1a" stroke="#555" stroke-width="1" rx="8"/>
      <text x="350" y="544" class="label" style="font-size:10px; fill:#888">Dropout (if training)</text>

      <line x1="350" y1="558" x2="350" y2="590" class="arrow"/>

      <!-- Skip connection 2 -->
      <line x1="200" y1="315" x2="130" y2="315" class="arrow-skip" style="marker-end:none"/>
      <line x1="130" y1="315" x2="130" y2="590" class="arrow-skip" style="marker-end:none"/>
      <line x1="130" y1="590" x2="200" y2="590" class="arrow-skip"/>
      <text x="80" y="450" style="fill:#4af7f7; font-size:9px; text-anchor:middle; font-weight:600">Residual</text>
      <text x="80" y="462" style="fill:#4af7f7; font-size:9px; text-anchor:middle; font-weight:600">Skip ②</text>
      <text x="80" y="477" style="fill:#4af7f7; font-size:8px; text-anchor:middle">∂L/∂x = 1</text>

      <!-- Add 2 -->
      <rect x="280" y="585" width="140" height="30" fill="#1a2e2e" stroke="#4af7f7" stroke-width="1.5" rx="8"/>
      <text x="350" y="600" class="label" style="font-size:11px">⊕ Add (x + FFN(x))</text>

      <line x1="350" y1="615" x2="350" y2="660" class="arrow"/>

      <!-- N× -->
      <rect x="630" y="95" width="70" height="540" rx="8" fill="none" stroke="#444" stroke-width="1" stroke-dasharray="4,4"/>
      <text x="665" y="370" class="label" style="font-size:13px; fill:#666" transform="rotate(-90,665,370)">× N Layers (e.g. 32, 80, 128)</text>

      <!-- Output -->
      <rect x="230" y="660" width="240" height="34" fill="#2e2a1a" stroke="#f7c94a" stroke-width="1.5" rx="8"/>
      <text x="350" y="677" class="label" style="font-size:12px">Final RMSNorm</text>

      <line x1="350" y1="694" x2="350" y2="725" class="arrow"/>

      <rect x="200" y="725" width="300" height="40" fill="#2e1a2e" stroke="#c74af7" stroke-width="1.5" rx="8"/>
      <text x="350" y="739" class="label" style="font-size:12px">Linear → Softmax</text>
      <text x="350" y="753" class="sublabel">Vocabulary logits → next token probability</text>

      <!-- ===== GRADIENT STABILITY BOX ===== -->
      <rect x="100" y="790" width="550" height="155" rx="10" fill="#0d0d15" stroke="#333" stroke-width="1"/>
      <text x="375" y="812" class="label" style="font-size:12px; fill:#4af7f7">Why 128-Layer Transformers Train Without Vanishing Gradients</text>

      <text x="130" y="838" style="fill:#4af7f7; font-size:11px; font-weight:600">① Residual Connections</text>
      <text x="340" y="838" style="fill:#aaa; font-size:10px">Identity gradient path ∂L/∂x = 1 bypasses all sublayers</text>

      <text x="130" y="858" style="fill:#f7c94a; font-size:11px; font-weight:600">② RMSNorm / LayerNorm</text>
      <text x="340" y="858" style="fill:#aaa; font-size:10px">Stabilizes activation distribution, prevents gradient explosion</text>

      <text x="130" y="878" style="fill:#4cf74a; font-size:11px; font-weight:600">③ Smooth Activations</text>
      <text x="340" y="878" style="fill:#aaa; font-size:10px">GELU/SiLU/SwiGLU: no dead neurons, continuous gradients</text>

      <text x="130" y="898" style="fill:#c74af7; font-size:11px; font-weight:600">④ Attention Structure</text>
      <text x="340" y="898" style="fill:#aaa; font-size:10px">Data-dependent, non-recurrent — no fixed weight multiplication across depth</text>

      <text x="130" y="918" style="fill:#888; font-size:11px; font-weight:600">⑤ Initialization</text>
      <text x="340" y="918" style="fill:#aaa; font-size:10px">Scaled init (1/√n) keeps variance stable across depth</text>

      <text x="130" y="940" style="fill:#f74a4a; font-size:9px; font-style:italic">Note: ReLU alone is NOT the reason. It was used in 2017 original but replaced due to dead neuron problem.</text>
    </svg>

    <div class="legend">
      <div class="legend-item"><div class="legend-color" style="background:#4a6cf7"></div>Multi-Head Attention (GQA)</div>
      <div class="legend-item"><div class="legend-color" style="background:#4cf74a"></div>SwiGLU FFN (Gated)</div>
      <div class="legend-item"><div class="legend-color" style="background:#f7c94a"></div>RMSNorm (Pre-LN)</div>
      <div class="legend-item"><div class="legend-color" style="background:#4af7f7"></div>Residual Connections</div>
      <div class="legend-item"><div class="legend-color" style="background:#c74af7"></div>Embedding / Output</div>
    </div>
  </div>

  <!-- ============ TAB 2: SwiGLU DETAIL ============ -->
  <div id="tab-swiglu" class="tab-content">
    <svg viewBox="0 0 750 520" xmlns="http://www.w3.org/2000/svg">
      <defs>
        <marker id="ah2" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto"><polygon points="0 0, 8 3, 0 6" fill="#555"/></marker>
      </defs>

      <text x="375" y="30" class="label" style="font-size:15px">SwiGLU FFN — Internal Structure</text>
      <text x="375" y="50" class="sublabel" style="font-size:11px">As used in LLaMA 2/3, Mistral, Gemma, PaLM 2</text>

      <!-- Input x -->
      <rect x="280" y="70" width="190" height="36" fill="#1a1a2e" stroke="#4a6cf7" stroke-width="1.5" rx="8"/>
      <text x="375" y="88" class="label" style="font-size:12px">Input x (d_model)</text>

      <!-- Split into two paths -->
      <line x1="330" y1="106" x2="200" y2="150" style="stroke:#555;stroke-width:1.5;marker-end:url(#ah2)"/>
      <line x1="420" y1="106" x2="550" y2="150" style="stroke:#555;stroke-width:1.5;marker-end:url(#ah2)"/>

      <!-- Gate path (left) -->
      <rect x="100" y="150" width="200" height="40" fill="#1a2e1a" stroke="#4cf74a" stroke-width="1.5" rx="8"/>
      <text x="200" y="164" class="label" style="font-size:11px">W_gate · x</text>
      <text x="200" y="178" class="sublabel">d_model → d_ff (⅔ × 4d)</text>

      <line x1="200" y1="190" x2="200" y2="230" style="stroke:#555;stroke-width:1.5;marker-end:url(#ah2)"/>

      <rect x="115" y="230" width="170" height="44" fill="#2e1a1a" stroke="#f7644a" stroke-width="1.5" rx="8"/>
      <text x="200" y="247" class="label" style="font-size:12px; fill:#ff8a6b">SiLU (Swish)</text>
      <text x="200" y="262" class="sublabel">σ(x) · x = x · sigmoid(x)</text>

      <text x="60" y="250" class="annotation" style="fill:#f7644a; font-size:9px">Smooth, non-monotonic</text>
      <text x="60" y="262" class="annotation" style="fill:#f7644a; font-size:9px">Allows negative values</text>
      <text x="60" y="274" class="annotation" style="fill:#f7644a; font-size:9px">No dead neurons</text>

      <!-- Up path (right) -->
      <rect x="450" y="150" width="200" height="40" fill="#1a2e1a" stroke="#4cf74a" stroke-width="1.5" rx="8"/>
      <text x="550" y="164" class="label" style="font-size:11px">W_up · x</text>
      <text x="550" y="178" class="sublabel">d_model → d_ff (⅔ × 4d)</text>

      <text x="550" y="212" class="sublabel" style="font-size:10px; fill:#888">Linear (no activation)</text>

      <!-- Element-wise multiply -->
      <line x1="200" y1="274" x2="200" y2="320" style="stroke:#555;stroke-width:1.5"/>
      <line x1="200" y1="320" x2="340" y2="340" style="stroke:#555;stroke-width:1.5;marker-end:url(#ah2)"/>
      <line x1="550" y1="190" x2="550" y2="320" style="stroke:#555;stroke-width:1.5"/>
      <line x1="550" y1="320" x2="410" y2="340" style="stroke:#555;stroke-width:1.5;marker-end:url(#ah2)"/>

      <rect x="315" y="335" width="120" height="36" fill="#2e2e1a" stroke="#f7f74a" stroke-width="1.5" rx="8"/>
      <text x="375" y="349" class="label" style="font-size:12px">⊙ Element</text>
      <text x="375" y="362" class="sublabel">Hadamard product</text>

      <text x="450" y="353" class="annotation" style="fill:#f7f74a">← This is the "Gated" part</text>
      <text x="450" y="367" class="annotation" style="fill:#f7f74a">   Gate controls information flow</text>

      <line x1="375" y1="371" x2="375" y2="410" style="stroke:#555;stroke-width:1.5;marker-end:url(#ah2)"/>

      <!-- Down projection -->
      <rect x="250" y="410" width="250" height="40" fill="#1a2e1a" stroke="#4cf74a" stroke-width="1.5" rx="8"/>
      <text x="375" y="424" class="label" style="font-size:11px">W_down · hidden</text>
      <text x="375" y="438" class="sublabel">d_ff → d_model</text>

      <line x1="375" y1="450" x2="375" y2="480" style="stroke:#555;stroke-width:1.5;marker-end:url(#ah2)"/>

      <rect x="280" y="480" width="190" height="30" fill="#1a1a2e" stroke="#4a6cf7" stroke-width="1.5" rx="8"/>
      <text x="375" y="495" class="label" style="font-size:11px">Output (d_model)</text>

      <!-- Formula -->
      <text x="375" y="40" class="sublabel" style="font-size:10px; fill:#aaa"></text>
      <rect x="80" y="60" width="10" height="1" fill="none"/>
    </svg>

    <div class="note">
      <p><strong>SwiGLU Formula:</strong> <code>FFN(x) = (SiLU(xW_gate) ⊙ xW_up) · W_down</code></p>
      <p style="margin-top:8px">
        Three weight matrices instead of two (vs original transformer's two). The <em>gating mechanism</em> lets the network learn 
        to selectively pass or suppress information — similar in spirit to LSTM gates but applied within FFN layers.
      </p>
      <p style="margin-top:8px">
        <strong>Why d_ff = ⅔ × 4d instead of 4d?</strong> Three matrices instead of two means ~50% more parameters at equal d_ff. 
        Setting d_ff to ⅔ × 4d keeps total parameter count equal to the original 2-matrix FFN while gaining the gating benefit.
      </p>
      <p style="margin-top:8px">
        <strong>Gradient flow through SwiGLU:</strong> The gate path provides smooth, non-zero gradients everywhere (SiLU has no flat regions). 
        The linear up-projection path provides an unobstructed gradient channel. Together they ensure stable training at 128+ layers.
      </p>
    </div>
  </div>

  <!-- ============ TAB 3: PRE-LN vs POST-LN ============ -->
  <div id="tab-compare" class="tab-content">
    <div class="comparison">
      <div>
        <h3 style="color:#f74a4a">Post-LN (Original 2017)</h3>
        <svg viewBox="0 0 320 380" xmlns="http://www.w3.org/2000/svg">
          <defs><marker id="ah3" markerWidth="7" markerHeight="5" refX="7" refY="2.5" orient="auto"><polygon points="0 0, 7 2.5, 0 5" fill="#555"/></marker></defs>
          <rect x="60" y="10" width="200" height="30" fill="#1a1a2e" stroke="#4a6cf7" stroke-width="1.2" rx="6"/>
          <text x="160" y="25" class="label" style="font-size:10px">Input x</text>
          
          <line x1="160" y1="40" x2="160" y2="60" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          
          <rect x="60" y="60" width="200" height="34" fill="#1a1a3e" stroke="#4a6cf7" stroke-width="1.2" rx="6"/>
          <text x="160" y="77" class="label" style="font-size:10px">Multi-Head Attention</text>
          
          <line x1="60" y1="30" x2="30" y2="30" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="30" x2="30" y2="110" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="110" x2="60" y2="110" class="arrow-skip" style="stroke-width:1;marker-end:url(#ah-skip)"/>
          
          <line x1="160" y1="94" x2="160" y2="105" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          <rect x="95" y="105" width="130" height="24" fill="#1a2e2e" stroke="#4af7f7" stroke-width="1" rx="6"/>
          <text x="160" y="117" class="label" style="font-size:9px">⊕ Add</text>
          
          <line x1="160" y1="129" x2="160" y2="145" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          <rect x="75" y="145" width="170" height="28" fill="#2e2a1a" stroke="#f7c94a" stroke-width="1.2" rx="6"/>
          <text x="160" y="159" class="label" style="font-size:10px; fill:#f7c94a">LayerNorm ← AFTER</text>
          
          <line x1="160" y1="173" x2="160" y2="195" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          
          <rect x="60" y="195" width="200" height="34" fill="#1a2e1a" stroke="#4cf74a" stroke-width="1.2" rx="6"/>
          <text x="160" y="212" class="label" style="font-size:10px">FFN (ReLU)</text>
          
          <line x1="60" y1="168" x2="30" y2="168" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="168" x2="30" y2="245" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="245" x2="60" y2="245" class="arrow-skip" style="stroke-width:1;marker-end:url(#ah-skip)"/>
          
          <line x1="160" y1="229" x2="160" y2="240" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          <rect x="95" y="240" width="130" height="24" fill="#1a2e2e" stroke="#4af7f7" stroke-width="1" rx="6"/>
          <text x="160" y="252" class="label" style="font-size:9px">⊕ Add</text>
          
          <line x1="160" y1="264" x2="160" y2="280" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          <rect x="75" y="280" width="170" height="28" fill="#2e2a1a" stroke="#f7c94a" stroke-width="1.2" rx="6"/>
          <text x="160" y="294" class="label" style="font-size:10px; fill:#f7c94a">LayerNorm ← AFTER</text>
          
          <text x="160" y="330" style="fill:#f74a4a; font-size:9px; text-anchor:middle; font-weight:600">⚠ Gradient must pass through LN</text>
          <text x="160" y="344" style="fill:#f74a4a; font-size:9px; text-anchor:middle">Increasingly unstable beyond ~24–48 layers</text>
          <text x="160" y="358" style="fill:#888; font-size:9px; text-anchor:middle">Requires warmup + careful LR scheduling</text>
          <text x="160" y="372" style="fill:#888; font-size:8px; text-anchor:middle">Used: BERT, Original Transformer</text>
        </svg>
      </div>
      <div>
        <h3 style="color:#4cf74a">Pre-LN (Modern Standard)</h3>
        <svg viewBox="0 0 320 380" xmlns="http://www.w3.org/2000/svg">
          <rect x="60" y="10" width="200" height="30" fill="#1a1a2e" stroke="#4a6cf7" stroke-width="1.2" rx="6"/>
          <text x="160" y="25" class="label" style="font-size:10px">Input x</text>
          
          <line x1="160" y1="40" x2="160" y2="55" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          
          <rect x="75" y="55" width="170" height="28" fill="#2e2a1a" stroke="#f7c94a" stroke-width="1.2" rx="6"/>
          <text x="160" y="69" class="label" style="font-size:10px; fill:#4cf74a">RMSNorm ← BEFORE</text>
          
          <line x1="160" y1="83" x2="160" y2="100" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          
          <rect x="60" y="100" width="200" height="34" fill="#1a1a3e" stroke="#4a6cf7" stroke-width="1.2" rx="6"/>
          <text x="160" y="117" class="label" style="font-size:10px">Multi-Head Attention (GQA)</text>
          
          <line x1="60" y1="25" x2="30" y2="25" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="25" x2="30" y2="150" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="150" x2="60" y2="150" class="arrow-skip" style="stroke-width:1;marker-end:url(#ah-skip)"/>
          
          <line x1="160" y1="134" x2="160" y2="145" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          <rect x="95" y="145" width="130" height="24" fill="#1a2e2e" stroke="#4af7f7" stroke-width="1" rx="6"/>
          <text x="160" y="157" class="label" style="font-size:9px">⊕ Add</text>
          
          <line x1="160" y1="169" x2="160" y2="190" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          
          <rect x="75" y="190" width="170" height="28" fill="#2e2a1a" stroke="#f7c94a" stroke-width="1.2" rx="6"/>
          <text x="160" y="204" class="label" style="font-size:10px; fill:#4cf74a">RMSNorm ← BEFORE</text>
          
          <line x1="160" y1="218" x2="160" y2="238" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          
          <rect x="60" y="238" width="200" height="34" fill="#1a2e1a" stroke="#4cf74a" stroke-width="1.2" rx="6"/>
          <text x="160" y="255" class="label" style="font-size:10px">FFN (SwiGLU)</text>
          
          <line x1="60" y1="168" x2="30" y2="168" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="168" x2="30" y2="290" class="arrow-skip" style="marker-end:none;stroke-width:1"/>
          <line x1="30" y1="290" x2="60" y2="290" class="arrow-skip" style="stroke-width:1;marker-end:url(#ah-skip)"/>
          
          <line x1="160" y1="272" x2="160" y2="285" style="stroke:#555;stroke-width:1.2;marker-end:url(#ah3)"/>
          <rect x="95" y="285" width="130" height="24" fill="#1a2e2e" stroke="#4af7f7" stroke-width="1" rx="6"/>
          <text x="160" y="297" class="label" style="font-size:9px">⊕ Add</text>
          
          <text x="160" y="330" style="fill:#4cf74a; font-size:9px; text-anchor:middle; font-weight:600">✓ Residual skip bypasses Norm entirely</text>
          <text x="160" y="344" style="fill:#4cf74a; font-size:9px; text-anchor:middle">Stable at 128+ layers</text>
          <text x="160" y="358" style="fill:#888; font-size:9px; text-anchor:middle">No warmup required, simpler training</text>
          <text x="160" y="372" style="fill:#888; font-size:8px; text-anchor:middle">Used: GPT-3, LLaMA, Mistral, PaLM, Gemma</text>
        </svg>
      </div>
    </div>

    <div class="note">
      <p><strong>Key architectural difference:</strong> In Post-LN, the residual path goes through LayerNorm, which can distort gradients at depth. 
      In Pre-LN, the residual connection <em>completely bypasses</em> normalization — providing an unobstructed identity gradient path (∂L/∂x = 1) 
      from the loss all the way back to the embedding layer.</p>
      <p style="margin-top:8px"><em>RMSNorm</em> replaces LayerNorm in most modern architectures (LLaMA, Mistral, Gemma). It drops the mean-centering step, 
      keeping only the variance normalization: <code>RMSNorm(x) = x / √(mean(x²) + ε) · γ</code>. Cheaper to compute, empirically equivalent.</p>
    </div>
  </div>

  <!-- ============ TAB 4: ACTIVATION FUNCTIONS ============ -->
  <div id="tab-activations" class="tab-content">
    <table class="model-table">
      <thead>
        <tr><th>Model Family</th><th>Year</th><th>Activation</th><th>LN Position</th><th>Attention</th><th>Notes</th></tr>
      </thead>
      <tbody>
        <tr><td>Original Transformer</td><td>2017</td><td class="outdated">ReLU</td><td>Post-LN</td><td>MHA</td><td>Vaswani et al. — foundational</td></tr>
        <tr><td>BERT</td><td>2018</td><td class="highlight">GELU</td><td>Post-LN</td><td>MHA</td><td>First major departure from ReLU</td></tr>
        <tr><td>GPT-2</td><td>2019</td><td class="highlight">GELU</td><td>Pre-LN</td><td>MHA</td><td>Pre-LN for training stability</td></tr>
        <tr><td>GPT-3</td><td>2020</td><td class="highlight">GELU</td><td>Pre-LN</td><td>MHA</td><td>175B params, Pre-LN for stability</td></tr>
        <tr><td>PaLM</td><td>2022</td><td class="highlight">SwiGLU</td><td>Pre-LN</td><td>MHA</td><td>Gated FFN, parallel attention+FFN</td></tr>
        <tr><td>LLaMA 1</td><td>2023</td><td class="highlight">SwiGLU</td><td>Pre-RMSNorm</td><td>MHA + RoPE</td><td>RMSNorm, RoPE, no bias terms</td></tr>
        <tr><td>LLaMA 2</td><td>2023</td><td class="highlight">SwiGLU</td><td>Pre-RMSNorm</td><td>GQA + RoPE</td><td>Grouped-Query Attention added</td></tr>
        <tr><td>Mistral/Mixtral</td><td>2023</td><td class="highlight">SwiGLU</td><td>Pre-RMSNorm</td><td>GQA + SWA</td><td>Sliding Window Attention</td></tr>
        <tr><td>Gemma</td><td>2024</td><td class="highlight">GeGLU</td><td>Pre-RMSNorm</td><td>MQA/GQA</td><td>GELU-gated variant</td></tr>
        <tr><td>LLaMA 3</td><td>2024</td><td class="highlight">SwiGLU</td><td>Pre-RMSNorm</td><td>GQA + RoPE</td><td>128K context, 405B params</td></tr>
        <tr><td>DeepSeek V3</td><td>2024</td><td class="highlight">SwiGLU</td><td>Pre-RMSNorm</td><td>MLA</td><td>Multi-head Latent Attention, MoE</td></tr>
      </tbody>
    </table>

    <div class="note">
      <p><strong>Evolution summary:</strong></p>
      <p style="margin-top:6px"><span class="warn">ReLU (2017)</span> → Simple but causes dead neurons (gradient = 0 for negative inputs, permanently)</p>
      <p><strong>GELU (2018+)</strong> → Gaussian Error Linear Unit: smooth approximation, probabilistic gating. Used by BERT, GPT family.</p>
      <p><em>SiLU/Swish (2020+)</em> → <code>x · sigmoid(x)</code> — smooth, allows small negative outputs, self-gated</p>
      <p><strong>SwiGLU (2022+)</strong> → Gated linear unit with SiLU activation. Three weight matrices. Current SOTA standard for LLMs.</p>
      <p><strong>GeGLU</strong> → Same gated structure but uses GELU instead of SiLU. Used by some Google models.</p>
      <p style="margin-top:8px">
        <em>Key insight:</em> The shift from ReLU → GELU → SwiGLU is NOT primarily about vanishing gradients (residual connections already solved that). 
        It's about <strong>training dynamics</strong>: smoother loss landscapes, better gradient flow through the FFN, 
        information gating, and avoiding dead neurons at scale. SwiGLU empirically improves perplexity by ~0.5-1.0 on language modeling benchmarks 
        with equivalent parameter counts.
      </p>
    </div>
  </div>

</div>

<script>
function showTab(name) {
  document.querySelectorAll('.tab-content').forEach(t => t.classList.remove('active'));
  document.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));
  document.getElementById('tab-' + name).classList.add('active');
  event.target.classList.add('active');
}
</script>
<div style="text-align:center;padding:24px 20px 32px;border-top:1px solid rgba(255,255,255,.06);margin-top:40px;font-family:monospace;font-size:10px;color:rgba(255,255,255,.25);letter-spacing:.5px">© 2025–2026 Sam Pooni · CS²B Research · <a href="https://sampooni.github.io/cssquaredb" style="color:rgba(94,234,212,.4);text-decoration:none">sampooni.github.io/cssquaredb</a> · All rights reserved</div>
</body>
</html>
